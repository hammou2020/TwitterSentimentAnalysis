{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = 'C:\\\\Users\\\\shrey\\\\Desktop\\\\ML-DL-NLP\\\\Projects\\\\twitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31962, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.ð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation\n",
       "5   6      0  [2/2] huge fan fare and big talking before the...\n",
       "6   7      0   @user camping tomorrow @user @user @user @use...\n",
       "7   8      0  the next school year is the year for exams.ð...\n",
       "8   9      0  we won!!! love the land!!! #allin #cavs #champ...\n",
       "9  10      0   @user @user welcome here !  i'm   it's so #gr..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(f'{my_path}/train/train_E6oV3lV.csv')\n",
    "\n",
    "print(train.shape)\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  word_count\n",
       "0   @user when a father is dysfunctional and is s...          21\n",
       "1  @user @user thanks for #lyft credit i can't us...          22\n",
       "2                                bihday your majesty           5\n",
       "3  #model   i love u take with u all the time in ...          17\n",
       "4             factsguide: society now    #motivation           8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of Words\n",
    "\n",
    "train['word_count'] = train['tweet'].apply(lambda x: len(str(x).split(\" \"))) # split and get the length of the sentence when whitespace is found\n",
    "train[['tweet','word_count']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>21</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>22</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>17</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  word_count  char_count\n",
       "0   @user when a father is dysfunctional and is s...          21         102\n",
       "1  @user @user thanks for #lyft credit i can't us...          22         122\n",
       "2                                bihday your majesty           5          21\n",
       "3  #model   i love u take with u all the time in ...          17          86\n",
       "4             factsguide: society now    #motivation           8          39"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of Characters\n",
    "train['char_count'] = train['tweet'].str.len() ## this also includes spaces\n",
    "train[['tweet','word_count', 'char_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  char_count\n",
       "0   @user when a father is dysfunctional and is s...         102\n",
       "1  @user @user thanks for #lyft credit i can't us...         122\n",
       "2                                bihday your majesty          21\n",
       "3  #model   i love u take with u all the time in ...          86\n",
       "4             factsguide: society now    #motivation          39"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['char_count'] = train['tweet'].str.len() ## this also includes spaces\n",
    "train[['tweet','char_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>4.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>5.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>5.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>4.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  avg_word\n",
       "0   @user when a father is dysfunctional and is s...  4.555556\n",
       "1  @user @user thanks for #lyft credit i can't us...  5.315789\n",
       "2                                bihday your majesty  5.666667\n",
       "3  #model   i love u take with u all the time in ...  4.928571\n",
       "4             factsguide: society now    #motivation  8.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"We will also extract another feature which will calculate the average word length of each tweet. \n",
    "This can also potentially help us in improving our model.\n",
    "\n",
    "Here, we simply take the sum of the length of all the words and divide it by the total length of the tweet:\"\"\"\n",
    "\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(x) for x in words)/len(words))\n",
    "\n",
    "train['avg_word'] = train['tweet'].apply(lambda x: avg_word(x))\n",
    "train[['tweet','avg_word']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  stopwords\n",
       "0   @user when a father is dysfunctional and is s...         10\n",
       "1  @user @user thanks for #lyft credit i can't us...          5\n",
       "2                                bihday your majesty          1\n",
       "3  #model   i love u take with u all the time in ...          5\n",
       "4             factsguide: society now    #motivation          1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Number of stopwords\n",
    "\n",
    "Generally, while solving an NLP problem, the first thing we do is to remove the stopwords.\n",
    "But sometimes calculating the number of stopwords can also give us some extra information which we might have been losing before.\n",
    "\n",
    "Here, we have imported stopwords from NLTK, which is a basic NLP library in python.\"\"\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "train['stopwords'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "train[['tweet','stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>hastags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  hastags\n",
       "0   @user when a father is dysfunctional and is s...        1\n",
       "1  @user @user thanks for #lyft credit i can't us...        3\n",
       "2                                bihday your majesty        0\n",
       "3  #model   i love u take with u all the time in ...        1\n",
       "4             factsguide: society now    #motivation        1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Number of special characters\n",
    "\n",
    "One more interesting feature which we can extract from a tweet is calculating the number of \n",
    "hashtags or mentions present in it. This also helps in extracting extra information from our text data.\n",
    "\n",
    "Here, we make use of the ‘starts with’ function because hashtags (or mentions) always appear at the beginning of a word.\"\"\"\n",
    "\n",
    "train['hastags'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "train[['tweet','hastags']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>numerics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  numerics\n",
       "0   @user when a father is dysfunctional and is s...         0\n",
       "1  @user @user thanks for #lyft credit i can't us...         0\n",
       "2                                bihday your majesty         0\n",
       "3  #model   i love u take with u all the time in ...         0\n",
       "4             factsguide: society now    #motivation         0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Number of numerics\n",
    "\n",
    "Just like we calculated the number of words, we can also calculate the number of numerics which are present in the tweets. \n",
    "It does not have a lot of use, but this is still a useful feature that should be run while doing similar exercises.\"\"\" \n",
    "\n",
    "train['numerics'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "train[['tweet','numerics']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  upper\n",
       "0   @user when a father is dysfunctional and is s...      0\n",
       "1  @user @user thanks for #lyft credit i can't us...      0\n",
       "2                                bihday your majesty      0\n",
       "3  #model   i love u take with u all the time in ...      0\n",
       "4             factsguide: society now    #motivation      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Number of Uppercase words\n",
    "\n",
    "Anger or rage is quite often expressed by writing in UPPERCASE words which makes this \n",
    "a necessary operation to identify those words.'''\n",
    "\n",
    "train['upper'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "train[['tweet','upper']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @user when a father is dysfunctional and is so...\n",
       "1    @user @user thanks for #lyft credit i can't us...\n",
       "2                                  bihday your majesty\n",
       "3    #model i love u take with u all the time in ur...\n",
       "4                  factsguide: society now #motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Lower case\n",
    "\n",
    "The first pre-processing step which we will do is transform our tweets into lower case. \n",
    "This avoids having multiple copies of the same words. For example, while calculating the word count, \n",
    "‘Analytics’ and ‘analytics’ will be taken as different words.\"\"\"\n",
    "\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    user when a father is dysfunctional and is so ...\n",
       "1    user user thanks for lyft credit i cant use ca...\n",
       "2                                  bihday your majesty\n",
       "3    model i love u take with u all the time in urð...\n",
       "4                    factsguide society now motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Removing Punctuation\n",
    "\n",
    "The next step is to remove punctuation, as it doesn’t add any extra information while treating text data. \n",
    "Therefore removing all instances of it will help us reduce the size of the training data.'''\n",
    "\n",
    "train['tweet'] = train['tweet'].str.replace('[^\\w\\s]','')\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    user father dysfunctional selfish drags kids d...\n",
       "1    user user thanks lyft credit cant use cause do...\n",
       "2                                       bihday majesty\n",
       "3                model love u take u time urð ðððð ððð\n",
       "4                        factsguide society motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Removal of Stop Words\n",
    "\n",
    "As we discussed earlier, stop words (or commonly occurring words) should be removed from the text data. \n",
    "For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries.\"\"\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user     17473\n",
       "love      2647\n",
       "ð         2511\n",
       "day       2199\n",
       "â         1797\n",
       "happy     1663\n",
       "amp       1582\n",
       "im        1139\n",
       "u         1136\n",
       "time      1110\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"\"\"Common word removal\n",
    "\n",
    "Previously, we just removed commonly occurring words in a general sense. \n",
    "We can also remove commonly occurring words from our text data First, let’s check the \n",
    "10 most frequently occurring words in our text data then take call to remove or retain.\"\"\"\n",
    "\n",
    "freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[:10] #Naive Method\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunctional selfish drags kids dysfun...\n",
       "1    thanks lyft credit cant use cause dont offer w...\n",
       "2                                       bihday majesty\n",
       "3                              model take urð ðððð ððð\n",
       "4                        factsguide society motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "soiree               1\n",
       "ððððððððmaternity    1\n",
       "carriefisher         1\n",
       "fulk                 1\n",
       "nuff                 1\n",
       "ufc207               1\n",
       "mit                  1\n",
       "bihdaycelebration    1\n",
       "thatd                1\n",
       "tucked               1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Rare words removal\n",
    "\n",
    "Similarly, just as we removed the most common words, this time let’s remove rarely \n",
    "occurring words from the text. Because they’re so rare, the association between them and \n",
    "other words is dominated by noise. You can replace rare words with a more general form and \n",
    "then this will have higher counts\"\"\"\n",
    "\n",
    "freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunctional selfish drags kids dysfun...\n",
       "1    thanks lyft credit cant use cause dont offer w...\n",
       "2                                       bihday majesty\n",
       "3                              model take urð ðððð ððð\n",
       "4                        factsguide society motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunctional selfish drags kiss dysfun...\n",
       "1    thanks left credit can use cause dont offer wh...\n",
       "2                                       midday majesty\n",
       "3                               model take or ðððð ððð\n",
       "4                        factsguide society motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Spelling correction\n",
    "\n",
    "We’ve all seen tweets with a plethora of spelling mistakes. Our timelines are often filled \n",
    "with hastly sent tweets that are barely legible at times.\n",
    "\n",
    "In that regard, spelling correction is a useful pre-processing step because this also \n",
    "will help us in reducing multiple copies of words. For example, “Analytics” and “analytcs” \n",
    "will be treated as different words even if they are used in the same sense.\n",
    "\n",
    "To achieve this we will use the textblob library. If you are not familiar with it, \n",
    "you can check my previous article on ‘NLP for beginners using textblob’.\n",
    "\"\"\"\n",
    "\n",
    "from textblob import TextBlob\n",
    "train['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['father', 'dysfunctional', 'selfish', 'drags', 'kids', 'dysfunction', 'run'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Tokenization\n",
    "\n",
    "Tokenization refers to dividing the text into a sequence of words or sentences. \n",
    "In our example, we have used the textblob library to first transform our tweets into a blob and then converted them into a series of words.\"\"\"\n",
    "\n",
    "TextBlob(train['tweet'][0]).words #check for all the tokens of 1st tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            father dysfunct selfish drag kid dysfunct run\n",
       "1        thank lyft credit cant use caus dont offer whe...\n",
       "2                                           bihday majesti\n",
       "3                                  model take urð ðððð ððð\n",
       "4                                  factsguid societi motiv\n",
       "5        22 huge fan fare big talk leav chao pay disput...\n",
       "6                                     camp tomorrow dannyâ\n",
       "7        next school year year examsð cant think school...\n",
       "8        land allin cav champion cleveland clevelandcavali\n",
       "9                                               welcom gr8\n",
       "10       ireland consum price index mom climb previou 0...\n",
       "11       selfish orlando standwithorlando pulseshoot or...\n",
       "12                      get see daddi today 80day gettingf\n",
       "13       cnn call michigan middl school build wall chan...\n",
       "14       comment australia opkillingbay seashepherd hel...\n",
       "15                ouchjunior angryðgot7 junior yugyoem omg\n",
       "16                                 thank paner thank posit\n",
       "17                                            retweet agre\n",
       "18             friday smile around via ig cooki make peopl\n",
       "19                            know essenti oil made chemic\n",
       "20       euro2016 peopl blame ha conced goal fat rooney...\n",
       "21       sad littl dude badday coneofsham cat piss funn...\n",
       "22            product man wine tool who weekend open drink\n",
       "23                                   lumpi say prove lumpi\n",
       "24             tgif ff gamedev indiedev indiegamedev squad\n",
       "25       beauti sign vendor 80 4500 upsideofflorida sho...\n",
       "26       smile media ðð pressconfer antalya turkey sund...\n",
       "27                  great panel mediat public servic ica16\n",
       "28                                             father ðððð\n",
       "29       50 peopl went nightclub good night 1 man actio...\n",
       "                               ...                        \n",
       "31932                                          thank gemma\n",
       "31933    judd homophob freemilo milo freemilo milo free...\n",
       "31934              ladi ban kentucki mall jcpenni kentucki\n",
       "31935    ugh tri enjoy hour drink talk polit bar guess ...\n",
       "31936    want know live life thing make less thing make...\n",
       "31937                                               island\n",
       "31938    fav actor vijaysethupathi fav actress fav dire...\n",
       "31939                                  whew product friday\n",
       "31940                                            she final\n",
       "31941    pass first year uni yay pass unistud photograp...\n",
       "31942                week fli humpday wednesday kamp ucsdâ\n",
       "31943         model photoshoot friday yay model follow emo\n",
       "31944             your surround peopl even deserv yet hate\n",
       "31945                feel like ððð dog summer hot help sun\n",
       "31946     omfg offend mailbox proud mailboxprid liberalism\n",
       "31947    dont ball hashtag say weasel away lumpi toni d...\n",
       "31948                    make ask anybodi god oh thank god\n",
       "31949    hear one new song dont go kati elli youtub ori...\n",
       "31950        tri tail us stop butt good goldenretriev anim\n",
       "31951                ive post new blog secondlif lone neko\n",
       "31952                                             went far\n",
       "31953    good morn instagram shower water berlin berlin...\n",
       "31954           holiday bull domin bull direct whatev want\n",
       "31955    less 2 week ðð¼ð¹ððµ ibizabringitonmallorcahol...\n",
       "31956                fish tomorrow carnt wait first 2 year\n",
       "31957                             ate isz youuuðððððððððâï\n",
       "31958    see nina turner airwav tri wrap mantl genuin h...\n",
       "31959             listen sad song monday morn otw work sad\n",
       "31960          sikh templ vandalis calgari wso condemn act\n",
       "31961                                         thank follow\n",
       "Name: tweet, Length: 31962, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Stemming\n",
    "\n",
    "Stemming refers to the removal of suffices, like “ing”, “ly”, “s”, etc. \n",
    "by a simple rule-based approach. For this purpose, we will use PorterStemmer from the \n",
    "NLTK library.\"\"\"\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "train['tweet'][:].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunctional selfish drag kid dysfunct...\n",
       "1    thanks lyft credit cant use cause dont offer w...\n",
       "2                                       bihday majesty\n",
       "3                              model take urð ðððð ððð\n",
       "4                        factsguide society motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"\"\"Lemmatization\n",
    "\n",
    "Lemmatization is a more effective option than stemming because it converts the word into \n",
    "its root word, rather than just stripping the suffices. \n",
    "It makes use of the vocabulary and does a morphological analysis to obtain the root word.\n",
    "Therefore, we usually prefer using lemmatization over stemming.\"\"\"\n",
    "\n",
    "from textblob import Word\n",
    "train['tweet'] = train['tweet'][:].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['father', 'dysfunctional']),\n",
       " WordList(['dysfunctional', 'selfish']),\n",
       " WordList(['selfish', 'drag']),\n",
       " WordList(['drag', 'kid']),\n",
       " WordList(['kid', 'dysfunction']),\n",
       " WordList(['dysfunction', 'run'])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"N-grams\n",
    "\n",
    "N-grams are the combination of multiple words used together. Ngrams with N=1 are called \n",
    "unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used.\n",
    "\n",
    "Unigrams do not usually contain as much information as compared to bigrams and trigrams. \n",
    "The basic principle behind n-grams is that they capture the language structure, \n",
    "like what letter or word is likely to follow the given one. The longer the n-gram \n",
    "(the higher the n), the more context you have to work with. Optimum length really depends \n",
    "on the application – if your n-grams are too short, you may fail to capture important \n",
    "differences. On the other hand, if they are too long, you may fail to capture the \n",
    "“general knowledge” and only stick to particular cases.\n",
    "\n",
    "So, let’s quickly extract bigrams from our tweets using the ngrams function of the textblob library.\"\"\"\n",
    "\n",
    "TextBlob(train['tweet'][0]).ngrams(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>selfish</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>father</td>\n",
       "      <td>903.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dysfunction</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drag</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dysfunctional</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>run</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kid</td>\n",
       "      <td>257.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>use</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>van</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cant</td>\n",
       "      <td>802.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>credit</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>getthanked</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dont</td>\n",
       "      <td>718.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>wheelchair</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pdx</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>disapointed</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>offer</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cause</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lyft</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>thanks</td>\n",
       "      <td>301.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>majesty</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bihday</td>\n",
       "      <td>825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ðððð</td>\n",
       "      <td>476.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>urð</td>\n",
       "      <td>325.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>model</td>\n",
       "      <td>376.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ððð</td>\n",
       "      <td>796.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>take</td>\n",
       "      <td>672.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>society</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>factsguide</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>motivation</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42660</th>\n",
       "      <td>jumpedtheshark</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42661</th>\n",
       "      <td>a1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42662</th>\n",
       "      <td>satanic</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42663</th>\n",
       "      <td>abomination2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42664</th>\n",
       "      <td>freemilo</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42665</th>\n",
       "      <td>judd</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42666</th>\n",
       "      <td>vijaysethupathi</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42667</th>\n",
       "      <td>whew</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42668</th>\n",
       "      <td>unistudent</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42669</th>\n",
       "      <td>photographystudent</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42670</th>\n",
       "      <td>ucs</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42671</th>\n",
       "      <td>ucsdâ</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42672</th>\n",
       "      <td>kamp</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42673</th>\n",
       "      <td>mailboxpride</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42674</th>\n",
       "      <td>liberalisme</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42675</th>\n",
       "      <td>dipshit</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42676</th>\n",
       "      <td>ellie</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42677</th>\n",
       "      <td>bern</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42678</th>\n",
       "      <td>zã¼rich</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42679</th>\n",
       "      <td>genf</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42680</th>\n",
       "      <td>berlincitygirl</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42681</th>\n",
       "      <td>ðð¼ð¹ððµ</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42682</th>\n",
       "      <td>ibizabringitonmallorcaholidayssummer</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42683</th>\n",
       "      <td>carnt</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42684</th>\n",
       "      <td>youuuðððððððððâï</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42685</th>\n",
       "      <td>isz</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42686</th>\n",
       "      <td>chisolm</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42687</th>\n",
       "      <td>shirley</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42688</th>\n",
       "      <td>mantle</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42689</th>\n",
       "      <td>airwave</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42690 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      words     tf\n",
       "0                                   selfish   10.0\n",
       "1                                    father  903.0\n",
       "2                               dysfunction    3.0\n",
       "3                                      drag    9.0\n",
       "4                             dysfunctional    1.0\n",
       "5                                       run  117.0\n",
       "6                                       kid  257.0\n",
       "7                                       use  114.0\n",
       "8                                       van    7.0\n",
       "9                                      cant  802.0\n",
       "10                                   credit   15.0\n",
       "11                               getthanked    2.0\n",
       "12                                     dont  718.0\n",
       "13                               wheelchair    3.0\n",
       "14                                      pdx    3.0\n",
       "15                              disapointed    1.0\n",
       "16                                    offer   36.0\n",
       "17                                    cause   78.0\n",
       "18                                     lyft    5.0\n",
       "19                                   thanks  301.0\n",
       "20                                  majesty   10.0\n",
       "21                                   bihday  825.0\n",
       "22                                     ðððð  476.0\n",
       "23                                      urð  325.0\n",
       "24                                    model  376.0\n",
       "25                                      ððð  796.0\n",
       "26                                     take  672.0\n",
       "27                                  society   30.0\n",
       "28                               factsguide   12.0\n",
       "29                               motivation  173.0\n",
       "...                                     ...    ...\n",
       "42660                        jumpedtheshark    1.0\n",
       "42661                                    a1    1.0\n",
       "42662                               satanic    1.0\n",
       "42663                          abomination2    1.0\n",
       "42664                              freemilo    5.0\n",
       "42665                                  judd    1.0\n",
       "42666                       vijaysethupathi    1.0\n",
       "42667                                  whew    1.0\n",
       "42668                            unistudent    1.0\n",
       "42669                    photographystudent    1.0\n",
       "42670                                   ucs    1.0\n",
       "42671                                 ucsdâ    1.0\n",
       "42672                                  kamp    1.0\n",
       "42673                          mailboxpride    1.0\n",
       "42674                           liberalisme    1.0\n",
       "42675                               dipshit    1.0\n",
       "42676                                 ellie    1.0\n",
       "42677                                  bern    1.0\n",
       "42678                               zã¼rich    1.0\n",
       "42679                                  genf    1.0\n",
       "42680                        berlincitygirl    1.0\n",
       "42681                              ðð¼ð¹ððµ    1.0\n",
       "42682  ibizabringitonmallorcaholidayssummer    1.0\n",
       "42683                                 carnt    1.0\n",
       "42684                      youuuðððððððððâï    1.0\n",
       "42685                                   isz    1.0\n",
       "42686                               chisolm    1.0\n",
       "42687                               shirley    1.0\n",
       "42688                                mantle    1.0\n",
       "42689                               airwave    1.0\n",
       "\n",
       "[42690 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Term frequency\n",
    "\n",
    "Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.\n",
    "\n",
    "Therefore, we can generalize term frequency as:\n",
    "\n",
    "TF = (Number of times term T appears in the particular row) / (number of terms in that row)\n",
    "\n",
    "Below, I have tried to show you the term frequency table of a tweet.\n",
    "\n",
    "Scikit-learn’s Tfidftransformer and Tfidfvectorizer aim to do the same thing, \n",
    "which is to convert a collection of raw documents to a matrix of TF-IDF features\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tf1 = (train['tweet'][:]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>selfish</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.664253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>father</td>\n",
       "      <td>903.0</td>\n",
       "      <td>3.475609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dysfunction</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.679156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drag</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.346951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dysfunctional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>run</td>\n",
       "      <td>117.0</td>\n",
       "      <td>4.642203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kid</td>\n",
       "      <td>257.0</td>\n",
       "      <td>4.567168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>use</td>\n",
       "      <td>114.0</td>\n",
       "      <td>3.552287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>van</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.236505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cant</td>\n",
       "      <td>802.0</td>\n",
       "      <td>3.538194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>credit</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.327781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>getthanked</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.679156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dont</td>\n",
       "      <td>718.0</td>\n",
       "      <td>3.745585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>wheelchair</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.273691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pdx</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.762865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>disapointed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>offer</td>\n",
       "      <td>36.0</td>\n",
       "      <td>6.522155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cause</td>\n",
       "      <td>78.0</td>\n",
       "      <td>5.690172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lyft</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.762865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>thanks</td>\n",
       "      <td>301.0</td>\n",
       "      <td>4.597751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>majesty</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.733246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bihday</td>\n",
       "      <td>825.0</td>\n",
       "      <td>3.720731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ðððð</td>\n",
       "      <td>476.0</td>\n",
       "      <td>3.490892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>urð</td>\n",
       "      <td>325.0</td>\n",
       "      <td>4.588478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>model</td>\n",
       "      <td>376.0</td>\n",
       "      <td>4.341618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ððð</td>\n",
       "      <td>796.0</td>\n",
       "      <td>2.916426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>take</td>\n",
       "      <td>672.0</td>\n",
       "      <td>3.761607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>society</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.788784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>factsguide</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.887396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>motivation</td>\n",
       "      <td>173.0</td>\n",
       "      <td>4.965131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42660</th>\n",
       "      <td>jumpedtheshark</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42661</th>\n",
       "      <td>a1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.664253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42662</th>\n",
       "      <td>satanic</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42663</th>\n",
       "      <td>abomination2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42664</th>\n",
       "      <td>freemilo</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42665</th>\n",
       "      <td>judd</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42666</th>\n",
       "      <td>vijaysethupathi</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42667</th>\n",
       "      <td>whew</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42668</th>\n",
       "      <td>unistudent</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42669</th>\n",
       "      <td>photographystudent</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42670</th>\n",
       "      <td>ucs</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.580544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42671</th>\n",
       "      <td>ucsdâ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42672</th>\n",
       "      <td>kamp</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.426393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42673</th>\n",
       "      <td>mailboxpride</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42674</th>\n",
       "      <td>liberalisme</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42675</th>\n",
       "      <td>dipshit</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.679156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42676</th>\n",
       "      <td>ellie</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.580544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42677</th>\n",
       "      <td>bern</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.421059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42678</th>\n",
       "      <td>zã¼rich</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42679</th>\n",
       "      <td>genf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42680</th>\n",
       "      <td>berlincitygirl</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42681</th>\n",
       "      <td>ðð¼ð¹ððµ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42682</th>\n",
       "      <td>ibizabringitonmallorcaholidayssummer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42683</th>\n",
       "      <td>carnt</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42684</th>\n",
       "      <td>youuuðððððððððâï</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42685</th>\n",
       "      <td>isz</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42686</th>\n",
       "      <td>chisolm</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42687</th>\n",
       "      <td>shirley</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42688</th>\n",
       "      <td>mantle</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.426393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42689</th>\n",
       "      <td>airwave</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42690 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      words     tf        idf\n",
       "0                                   selfish   10.0   7.664253\n",
       "1                                    father  903.0   3.475609\n",
       "2                               dysfunction    3.0   9.679156\n",
       "3                                      drag    9.0   6.346951\n",
       "4                             dysfunctional    1.0  10.372303\n",
       "5                                       run  117.0   4.642203\n",
       "6                                       kid  257.0   4.567168\n",
       "7                                       use  114.0   3.552287\n",
       "8                                       van    7.0   5.236505\n",
       "9                                      cant  802.0   3.538194\n",
       "10                                   credit   15.0   7.327781\n",
       "11                               getthanked    2.0   9.679156\n",
       "12                                     dont  718.0   3.745585\n",
       "13                               wheelchair    3.0   9.273691\n",
       "14                                      pdx    3.0   8.762865\n",
       "15                              disapointed    1.0  10.372303\n",
       "16                                    offer   36.0   6.522155\n",
       "17                                    cause   78.0   5.690172\n",
       "18                                     lyft    5.0   8.762865\n",
       "19                                   thanks  301.0   4.597751\n",
       "20                                  majesty   10.0   7.733246\n",
       "21                                   bihday  825.0   3.720731\n",
       "22                                     ðððð  476.0   3.490892\n",
       "23                                      urð  325.0   4.588478\n",
       "24                                    model  376.0   4.341618\n",
       "25                                      ððð  796.0   2.916426\n",
       "26                                     take  672.0   3.761607\n",
       "27                                  society   30.0   6.788784\n",
       "28                               factsguide   12.0   7.887396\n",
       "29                               motivation  173.0   4.965131\n",
       "...                                     ...    ...        ...\n",
       "42660                        jumpedtheshark    1.0  10.372303\n",
       "42661                                    a1    1.0   7.664253\n",
       "42662                               satanic    1.0  10.372303\n",
       "42663                          abomination2    1.0  10.372303\n",
       "42664                              freemilo    5.0  10.372303\n",
       "42665                                  judd    1.0  10.372303\n",
       "42666                       vijaysethupathi    1.0  10.372303\n",
       "42667                                  whew    1.0  10.372303\n",
       "42668                            unistudent    1.0  10.372303\n",
       "42669                    photographystudent    1.0  10.372303\n",
       "42670                                   ucs    1.0   8.580544\n",
       "42671                                 ucsdâ    1.0  10.372303\n",
       "42672                                  kamp    1.0   8.426393\n",
       "42673                          mailboxpride    1.0  10.372303\n",
       "42674                           liberalisme    1.0  10.372303\n",
       "42675                               dipshit    1.0   9.679156\n",
       "42676                                 ellie    1.0   8.580544\n",
       "42677                                  bern    1.0   6.421059\n",
       "42678                               zã¼rich    1.0  10.372303\n",
       "42679                                  genf    1.0  10.372303\n",
       "42680                        berlincitygirl    1.0  10.372303\n",
       "42681                              ðð¼ð¹ððµ    1.0  10.372303\n",
       "42682  ibizabringitonmallorcaholidayssummer    1.0  10.372303\n",
       "42683                                 carnt    1.0  10.372303\n",
       "42684                      youuuðððððððððâï    1.0  10.372303\n",
       "42685                                   isz    1.0  10.372303\n",
       "42686                               chisolm    1.0  10.372303\n",
       "42687                               shirley    1.0  10.372303\n",
       "42688                                mantle    1.0   8.426393\n",
       "42689                               airwave    1.0  10.372303\n",
       "\n",
       "[42690 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Inverse Document Frequency\n",
    "\n",
    "The intuition behind inverse document frequency (IDF) is that a word is not of much use \n",
    "to us if it’s appearing in all the documents.\n",
    "\n",
    "Therefore, the IDF of each word is the log of the ratio of the total number of \n",
    "rows to the number of rows in which that word is present.\n",
    "\n",
    "IDF = log(N/n), where, N is the total number of rows and n is the number of rows in \n",
    "which the word was present.\n",
    "\n",
    "Inverse Document Frequency (IDF): is a scoring of how rare the word is across documents. \n",
    "IDF is a measure of how rare a term is. Rarer the term, more is the IDF score.\n",
    "\n",
    "So, let’s calculate IDF for the same tweets for which we calculated the term frequency.\"\"\"\n",
    "\n",
    "for i,word in enumerate(tf1['words']):\n",
    "    tf1.loc[i, 'idf'] = np.log(train.shape[0]/(len(train[train['tweet'].str.contains(word)])))\n",
    "\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>selfish</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.664253</td>\n",
       "      <td>76.642528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>father</td>\n",
       "      <td>903.0</td>\n",
       "      <td>3.475609</td>\n",
       "      <td>3138.474606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dysfunction</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.679156</td>\n",
       "      <td>29.037467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drag</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.346951</td>\n",
       "      <td>57.122562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dysfunctional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>run</td>\n",
       "      <td>117.0</td>\n",
       "      <td>4.642203</td>\n",
       "      <td>543.137774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kid</td>\n",
       "      <td>257.0</td>\n",
       "      <td>4.567168</td>\n",
       "      <td>1173.762178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>use</td>\n",
       "      <td>114.0</td>\n",
       "      <td>3.552287</td>\n",
       "      <td>404.960674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>van</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.236505</td>\n",
       "      <td>36.655532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cant</td>\n",
       "      <td>802.0</td>\n",
       "      <td>3.538194</td>\n",
       "      <td>2837.631778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>credit</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.327781</td>\n",
       "      <td>109.916708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>getthanked</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.679156</td>\n",
       "      <td>19.358312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dont</td>\n",
       "      <td>718.0</td>\n",
       "      <td>3.745585</td>\n",
       "      <td>2689.330193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>wheelchair</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.273691</td>\n",
       "      <td>27.821072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pdx</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.762865</td>\n",
       "      <td>26.288595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>disapointed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>offer</td>\n",
       "      <td>36.0</td>\n",
       "      <td>6.522155</td>\n",
       "      <td>234.797593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cause</td>\n",
       "      <td>78.0</td>\n",
       "      <td>5.690172</td>\n",
       "      <td>443.833396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lyft</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.762865</td>\n",
       "      <td>43.814325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>thanks</td>\n",
       "      <td>301.0</td>\n",
       "      <td>4.597751</td>\n",
       "      <td>1383.923181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>majesty</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.733246</td>\n",
       "      <td>77.332456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bihday</td>\n",
       "      <td>825.0</td>\n",
       "      <td>3.720731</td>\n",
       "      <td>3069.603160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ðððð</td>\n",
       "      <td>476.0</td>\n",
       "      <td>3.490892</td>\n",
       "      <td>1661.664436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>urð</td>\n",
       "      <td>325.0</td>\n",
       "      <td>4.588478</td>\n",
       "      <td>1491.255283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>model</td>\n",
       "      <td>376.0</td>\n",
       "      <td>4.341618</td>\n",
       "      <td>1632.448261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ððð</td>\n",
       "      <td>796.0</td>\n",
       "      <td>2.916426</td>\n",
       "      <td>2321.475326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>take</td>\n",
       "      <td>672.0</td>\n",
       "      <td>3.761607</td>\n",
       "      <td>2527.799858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>society</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.788784</td>\n",
       "      <td>203.663521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>factsguide</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.887396</td>\n",
       "      <td>94.648756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>motivation</td>\n",
       "      <td>173.0</td>\n",
       "      <td>4.965131</td>\n",
       "      <td>858.967698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42660</th>\n",
       "      <td>jumpedtheshark</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42661</th>\n",
       "      <td>a1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.664253</td>\n",
       "      <td>7.664253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42662</th>\n",
       "      <td>satanic</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42663</th>\n",
       "      <td>abomination2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42664</th>\n",
       "      <td>freemilo</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>51.861515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42665</th>\n",
       "      <td>judd</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42666</th>\n",
       "      <td>vijaysethupathi</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42667</th>\n",
       "      <td>whew</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42668</th>\n",
       "      <td>unistudent</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42669</th>\n",
       "      <td>photographystudent</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42670</th>\n",
       "      <td>ucs</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.580544</td>\n",
       "      <td>8.580544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42671</th>\n",
       "      <td>ucsdâ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42672</th>\n",
       "      <td>kamp</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.426393</td>\n",
       "      <td>8.426393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42673</th>\n",
       "      <td>mailboxpride</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42674</th>\n",
       "      <td>liberalisme</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42675</th>\n",
       "      <td>dipshit</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.679156</td>\n",
       "      <td>9.679156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42676</th>\n",
       "      <td>ellie</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.580544</td>\n",
       "      <td>8.580544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42677</th>\n",
       "      <td>bern</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.421059</td>\n",
       "      <td>6.421059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42678</th>\n",
       "      <td>zã¼rich</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42679</th>\n",
       "      <td>genf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42680</th>\n",
       "      <td>berlincitygirl</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42681</th>\n",
       "      <td>ðð¼ð¹ððµ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42682</th>\n",
       "      <td>ibizabringitonmallorcaholidayssummer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42683</th>\n",
       "      <td>carnt</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42684</th>\n",
       "      <td>youuuðððððððððâï</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42685</th>\n",
       "      <td>isz</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42686</th>\n",
       "      <td>chisolm</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42687</th>\n",
       "      <td>shirley</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42688</th>\n",
       "      <td>mantle</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.426393</td>\n",
       "      <td>8.426393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42689</th>\n",
       "      <td>airwave</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42690 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      words     tf        idf        tfidf\n",
       "0                                   selfish   10.0   7.664253    76.642528\n",
       "1                                    father  903.0   3.475609  3138.474606\n",
       "2                               dysfunction    3.0   9.679156    29.037467\n",
       "3                                      drag    9.0   6.346951    57.122562\n",
       "4                             dysfunctional    1.0  10.372303    10.372303\n",
       "5                                       run  117.0   4.642203   543.137774\n",
       "6                                       kid  257.0   4.567168  1173.762178\n",
       "7                                       use  114.0   3.552287   404.960674\n",
       "8                                       van    7.0   5.236505    36.655532\n",
       "9                                      cant  802.0   3.538194  2837.631778\n",
       "10                                   credit   15.0   7.327781   109.916708\n",
       "11                               getthanked    2.0   9.679156    19.358312\n",
       "12                                     dont  718.0   3.745585  2689.330193\n",
       "13                               wheelchair    3.0   9.273691    27.821072\n",
       "14                                      pdx    3.0   8.762865    26.288595\n",
       "15                              disapointed    1.0  10.372303    10.372303\n",
       "16                                    offer   36.0   6.522155   234.797593\n",
       "17                                    cause   78.0   5.690172   443.833396\n",
       "18                                     lyft    5.0   8.762865    43.814325\n",
       "19                                   thanks  301.0   4.597751  1383.923181\n",
       "20                                  majesty   10.0   7.733246    77.332456\n",
       "21                                   bihday  825.0   3.720731  3069.603160\n",
       "22                                     ðððð  476.0   3.490892  1661.664436\n",
       "23                                      urð  325.0   4.588478  1491.255283\n",
       "24                                    model  376.0   4.341618  1632.448261\n",
       "25                                      ððð  796.0   2.916426  2321.475326\n",
       "26                                     take  672.0   3.761607  2527.799858\n",
       "27                                  society   30.0   6.788784   203.663521\n",
       "28                               factsguide   12.0   7.887396    94.648756\n",
       "29                               motivation  173.0   4.965131   858.967698\n",
       "...                                     ...    ...        ...          ...\n",
       "42660                        jumpedtheshark    1.0  10.372303    10.372303\n",
       "42661                                    a1    1.0   7.664253     7.664253\n",
       "42662                               satanic    1.0  10.372303    10.372303\n",
       "42663                          abomination2    1.0  10.372303    10.372303\n",
       "42664                              freemilo    5.0  10.372303    51.861515\n",
       "42665                                  judd    1.0  10.372303    10.372303\n",
       "42666                       vijaysethupathi    1.0  10.372303    10.372303\n",
       "42667                                  whew    1.0  10.372303    10.372303\n",
       "42668                            unistudent    1.0  10.372303    10.372303\n",
       "42669                    photographystudent    1.0  10.372303    10.372303\n",
       "42670                                   ucs    1.0   8.580544     8.580544\n",
       "42671                                 ucsdâ    1.0  10.372303    10.372303\n",
       "42672                                  kamp    1.0   8.426393     8.426393\n",
       "42673                          mailboxpride    1.0  10.372303    10.372303\n",
       "42674                           liberalisme    1.0  10.372303    10.372303\n",
       "42675                               dipshit    1.0   9.679156     9.679156\n",
       "42676                                 ellie    1.0   8.580544     8.580544\n",
       "42677                                  bern    1.0   6.421059     6.421059\n",
       "42678                               zã¼rich    1.0  10.372303    10.372303\n",
       "42679                                  genf    1.0  10.372303    10.372303\n",
       "42680                        berlincitygirl    1.0  10.372303    10.372303\n",
       "42681                              ðð¼ð¹ððµ    1.0  10.372303    10.372303\n",
       "42682  ibizabringitonmallorcaholidayssummer    1.0  10.372303    10.372303\n",
       "42683                                 carnt    1.0  10.372303    10.372303\n",
       "42684                      youuuðððððððððâï    1.0  10.372303    10.372303\n",
       "42685                                   isz    1.0  10.372303    10.372303\n",
       "42686                               chisolm    1.0  10.372303    10.372303\n",
       "42687                               shirley    1.0  10.372303    10.372303\n",
       "42688                                mantle    1.0   8.426393     8.426393\n",
       "42689                               airwave    1.0  10.372303    10.372303\n",
       "\n",
       "[42690 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Term Frequency – Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "TF-IDF is the multiplication of the TF and IDF which we calculated above.\"\"\"\n",
    "\n",
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<31962x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 120495 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"We can see that the TF-IDF has penalized words like ‘don’t’, ‘can’t’, and ‘use’ \n",
    "because they are commonly occurring words. However, it has given a high weight to \n",
    "“disappointed” since that will be very useful in determining the sentiment of the tweet.\n",
    "\n",
    "We don’t have to calculate TF and IDF every time beforehand and then multiply it to \n",
    "obtain TF-IDF. Instead, sklearn has a separate function to directly obtain it:\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    " stop_words= 'english',ngram_range=(1,2))\n",
    "train_vect = tfidf.fit_transform(train['tweet'])\n",
    "\n",
    "train_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<31962x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 128378 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Bag of Words\n",
    "\n",
    "Bag of Words (BoW) refers to the representation of text which describes the presence of \n",
    "words within the text data. The intuition behind this is that two similar text fields \n",
    "will contain similar kind of words, and will therefore have a similar bag of words. Further, that from the text alone we can learn something about the meaning of the document.\n",
    "\n",
    "For implementation, sklearn provides a separate function for it as shown below:\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(train['tweet'])\n",
    "train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (-0.3, 0.5354166666666667)\n",
       "1                    (0.2, 0.2)\n",
       "2                    (0.0, 0.0)\n",
       "3                    (0.0, 0.0)\n",
       "4                    (0.0, 0.0)\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sentiment Analysis\n",
    "\n",
    "If you recall, our problem was to detect the sentiment of the tweet.\n",
    "So, before applying any ML/DL models (which can have a separate feature detecting the \n",
    "sentiment using the textblob library), let’s check the sentiment of the first few tweets.\"\"\"\n",
    "\n",
    "train['tweet'][:5].apply(lambda x: TextBlob(x).sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>father dysfunctional selfish drag kid dysfunct...</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thanks lyft credit cant use cause dont offer w...</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday majesty</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model take urð ðððð ððð</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide society motivation</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment\n",
       "0  father dysfunctional selfish drag kid dysfunct...       -0.3\n",
       "1  thanks lyft credit cant use cause dont offer w...        0.2\n",
       "2                                     bihday majesty        0.0\n",
       "3                            model take urð ðððð ððð        0.0\n",
       "4                      factsguide society motivation        0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Above, you can see that it returns a tuple representing polarity and subjectivity of \n",
    "each tweet. Here, we only extract polarity as it indicates the sentiment as value nearer \n",
    "to 1 means a positive sentiment and values nearer to -1 means a negative sentiment. \n",
    "This can also work as a feature for building a machine learning model.\"\"\"\n",
    "\n",
    "train['sentiment'] = train['tweet'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "train[['tweet','sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_input_file = './glove.6B.50d.txt/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors # load the Stanford GloVe model\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.4828e-01,  1.7761e-01,  4.2346e-01, -3.1489e-01,  3.2273e-01,\n",
       "       -7.2413e-01, -7.8955e-01,  4.9214e-01, -2.0693e-01, -5.5088e-04,\n",
       "       -4.7877e-01,  2.8853e-01, -5.7376e-01,  2.7217e-01,  1.1129e+00,\n",
       "        5.7808e-01,  6.9321e-01, -2.8652e-01, -5.4545e-02, -6.1826e-01,\n",
       "        1.7227e-01,  2.9263e-01,  3.8184e-01,  6.2186e-01,  5.5461e-01,\n",
       "       -1.7411e+00, -2.8802e-01, -1.7140e-01,  7.4743e-01, -1.0135e+00,\n",
       "        3.3596e+00,  1.1370e+00, -1.0028e+00,  1.7685e-01, -6.1795e-03,\n",
       "       -6.3491e-02,  1.9077e-01,  4.4046e-02,  3.8228e-01, -4.1607e-01,\n",
       "       -5.0359e-01, -8.3803e-02,  1.7508e-01,  4.0420e-01,  7.7324e-02,\n",
       "        1.7415e-01,  1.2541e-01, -2.1820e-01,  1.2971e-01,  3.2953e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['go']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.34176  , -0.32715  ,  0.66209  , -0.71138  ,  0.28488  ,\n",
       "       -0.19242  , -0.85185  ,  0.56403  , -0.13852  , -0.06717  ,\n",
       "       -0.42702  , -0.20546  , -0.70012  , -0.13799  ,  0.29457  ,\n",
       "        0.1881   ,  0.50458  , -0.14432  , -0.73977  , -0.63253  ,\n",
       "        0.06105  ,  0.55907  ,  0.45083  ,  0.16689  ,  0.55929  ,\n",
       "       -1.924    ,  0.48437  ,  0.66656  ,  0.89432  , -1.0412   ,\n",
       "        3.1784   ,  1.0617   , -0.15902  ,  0.0067243, -0.35329  ,\n",
       "        0.39728  , -0.44211  ,  0.41718  ,  0.38365  , -0.39747  ,\n",
       "       -0.15511  ,  0.21717  ,  0.047058 ,  0.3904   , -0.20639  ,\n",
       "        0.075575 ,  0.09143  , -1.0418   ,  0.24466  , -1.1117   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['away']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49004   , -0.14953999,  1.0855501 , -1.02627   ,  0.60761   ,\n",
       "       -0.91655   , -1.6414    ,  1.05617   , -0.34544998, -0.06772088,\n",
       "       -0.90579   ,  0.08307   , -1.27388   ,  0.13418001,  1.40747   ,\n",
       "        0.76618   ,  1.19779   , -0.43084002, -0.794315  , -1.25079   ,\n",
       "        0.23332   ,  0.85169995,  0.83267   ,  0.78875005,  1.1139    ,\n",
       "       -3.6651    ,  0.19634998,  0.49515998,  1.6417501 , -2.0547    ,\n",
       "        6.538     ,  2.1987    , -1.1618199 ,  0.1835743 , -0.3594695 ,\n",
       "        0.333789  , -0.25134   ,  0.461226  ,  0.76593   , -0.81354   ,\n",
       "       -0.6587    ,  0.133367  ,  0.222138  ,  0.7946    , -0.12906599,\n",
       "        0.24972501,  0.21684   , -1.26      ,  0.37437   , -0.78217006],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model['go'] + model['away'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24502   , -0.07477   ,  0.54277503, -0.513135  ,  0.303805  ,\n",
       "       -0.458275  , -0.8207    ,  0.528085  , -0.17272499, -0.03386044,\n",
       "       -0.452895  ,  0.041535  , -0.63694   ,  0.06709   ,  0.703735  ,\n",
       "        0.38309   ,  0.598895  , -0.21542001, -0.3971575 , -0.625395  ,\n",
       "        0.11666   ,  0.42584997,  0.416335  ,  0.39437503,  0.55695   ,\n",
       "       -1.83255   ,  0.09817499,  0.24757999,  0.82087505, -1.02735   ,\n",
       "        3.269     ,  1.09935   , -0.58090997,  0.09178715, -0.17973475,\n",
       "        0.1668945 , -0.12567   ,  0.230613  ,  0.382965  , -0.40677   ,\n",
       "       -0.32935   ,  0.0666835 ,  0.111069  ,  0.3973    , -0.064533  ,\n",
       "        0.12486251,  0.10842   , -0.63      ,  0.187185  , -0.39108503],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model['go'] + model['away'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
